{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Fashion-MNIST\n",
    "\n",
    "Now it's your turn to build and train a neural network. You'll be using the [Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist), a drop-in replacement for the MNIST dataset. MNIST is actually quite trivial with neural networks where you can easily achieve better than 97% accuracy. Fashion-MNIST is a set of 28x28 greyscale images of clothes. It's more complex than MNIST, so it's a better representation of the actual performance of your network, and a better representation of datasets you'll use in the real world.\n",
    "\n",
    "<img src='assets/fashion-mnist-sprite.png' width=500px>\n",
    "\n",
    "In this notebook, you'll build your own neural network. For the most part, you could just copy and paste the code from Part 3, but you wouldn't be learning. It's important for you to write the code yourself and get it to work. Feel free to consult the previous notebooks though as you work through this.\n",
    "\n",
    "First off, let's load the dataset through torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import helper\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see one of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = next(iter(trainloader))\n",
    "helper.imshow(image[0,:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (7): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Network architecture.\n",
    "\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(784, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5193355804059043\n",
      "Training loss: 0.391364349771156\n",
      "Training loss: 0.3540137001891126\n",
      "Training loss: 0.3318180482445368\n",
      "Training loss: 0.31557746037745527\n",
      "Training loss: 0.3021811648869692\n",
      "Training loss: 0.2943737499797141\n",
      "Training loss: 0.28059691143855614\n",
      "Training loss: 0.2730604246107818\n",
      "Training loss: 0.2657000090378815\n",
      "Training loss: 0.26278182967448793\n",
      "Training loss: 0.2517578973651314\n"
     ]
    }
   ],
   "source": [
    "# Training the network.\n",
    "\n",
    "epochs = 15\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        \n",
    "        images = images.view(images.shape[0], -1)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "\n",
    "# Testing out.\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "img = images[0]\n",
    "\n",
    "img = img.resize_(1, 784)\n",
    "\n",
    "ps = torch.exp(model(img))\n",
    "\n",
    "# Plotting\n",
    "helper.view_classify(img.resize_(1, 28, 28), ps, version='Fashion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
